# Cloud Transcript ğŸµâ¡ï¸ğŸ“

Transcritor inteligente de conversas WhatsApp com **GPU local** (RTX 3060) e fallbacks para APIs cloud.

## âœ¨ Funcionalidades

- ğŸš€ **GPU Local**: Whisper rodando direto na RTX 3060 (ultra-rÃ¡pido!)
- ğŸ“ **Upload mÃºltiplo** de Ã¡udios (.opus, .mp3, .wav, .m4a)
- ğŸ¤– **MÃºltiplas engines**: Local GPU â†’ Groq â†’ Google Cloud â†’ HuggingFace
- âœï¸ **Editor visual** para inserÃ§Ã£o de contexto entre Ã¡udios
- ğŸ”„ **ReordenaÃ§Ã£o** drag & drop dos blocos de conversa
- ğŸ“¤ **ExportaÃ§Ã£o flexÃ­vel** (TXT, JSON, Markdown, Chat format)
- ğŸ³ **Docker + GPU** para desenvolvimento otimizado

## ğŸ› ï¸ Tecnologias

- **NVIDIA CUDA** + **PyTorch** - AceleraÃ§Ã£o GPU
- **OpenAI Whisper large-v3** - TranscriÃ§Ã£o local
- **Python 3.11** + **Streamlit** - Interface web
- **Docker + NVIDIA Container Runtime** - ContainerizaÃ§Ã£o GPU

## âš¡ ConfiguraÃ§Ãµes de Performance

### **Modo GPU Local (Recomendado)**
- **RTX 3060** - ~10x mais rÃ¡pido que APIs
- **Whisper large-v3** - Qualidade mÃ¡xima PT-BR
- **Sem limites** de uso ou rate limiting
- **Privacidade total** - nada sai da mÃ¡quina

### **Modo API Cloud (Fallback)**
- **Groq**: 6.000 segundos/minuto grÃ¡tis
- **Google Cloud**: 60 minutos/mÃªs grÃ¡tis
- **HuggingFace**: Rate limited

## ğŸš€ InÃ­cio RÃ¡pido

### PrÃ©-requisitos

- **NVIDIA RTX 3060** (ou GPU compatÃ­vel)
- **NVIDIA Container Toolkit** instalado
- **Docker Desktop** com GPU support
- **WSL2** (se Windows)

### ConfiguraÃ§Ã£o GPU (Primeira vez)

1. **Instale NVIDIA Container Toolkit**
   ```bash
   # Ubuntu/WSL2
   curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
   curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
     sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
     sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
   
   sudo apt-get update
   sudo apt-get install -y nvidia-container-toolkit
   sudo systemctl restart docker
   ```

2. **Teste GPU Docker**
   ```bash
   docker run --rm --gpus all nvidia/cuda:12.1-base-ubuntu22.04 nvidia-smi
   ```

### Executar com GPU

1. **Clone e configure**
   ```bash
   git clone https://github.com/ecodelearn/cloud_transcript.git
   cd cloud_transcript
   cp .env.example .env
   ```

2. **Configure modo GPU no .env**
   ```env
   TRANSCRIPTION_MODE=local_gpu
   USE_LOCAL_GPU=true
   WHISPER_MODEL=large-v3
   ```

3. **Execute com GPU** ğŸš€
   ```bash
   docker-compose -f docker-compose.gpu.yml up --build
   ```

4. **Acesse a aplicaÃ§Ã£o**
   ```
   http://localhost:8501
   ```

### Executar sem GPU (CPU/APIs)

```bash
# Modo tradicional com APIs cloud
docker-compose up --build
```

## ğŸ“‹ Comandos DisponÃ­veis

### **GPU Mode (Recomendado)**
```bash
# Iniciar com GPU
docker-compose -f docker-compose.gpu.yml up --build

# Background com GPU  
docker-compose -f docker-compose.gpu.yml up -d

# Logs GPU
docker-compose -f docker-compose.gpu.yml logs -f app

# Parar GPU
docker-compose -f docker-compose.gpu.yml down
```

### **CPU Mode**
```bash
# Iniciar CPU/API mode
docker-compose up --build

# Ver logs
docker-compose logs -f app

# Parar
docker-compose down
```

### **Monitoramento GPU**
```bash
# Ver uso GPU em tempo real
watch -n 1 nvidia-smi

# Logs GPU do container
docker exec -it cloud_transcript_gpu nvidia-smi
```

## ğŸ—ï¸ Arquitetura

```
cloud_transcript/
â”œâ”€â”€ src/                   # CÃ³digo fonte
â”‚   â”œâ”€â”€ app.py            # Interface Streamlit
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ transcription.py      # Multi-engine (GPU + APIs)
â”‚   â”‚   â”œâ”€â”€ whisper_gpu.py        # Whisper local GPU
â”‚   â”‚   â””â”€â”€ audio_processor.py    # Processamento Ã¡udio
â”‚   â””â”€â”€ components/               # Componentes UI
â”œâ”€â”€ docker-compose.yml           # CPU/API mode
â”œâ”€â”€ docker-compose.gpu.yml       # GPU mode
â”œâ”€â”€ Dockerfile                   # Base container
â”œâ”€â”€ Dockerfile.gpu               # GPU container
â”œâ”€â”€ requirements.txt             # CPU dependencies
â”œâ”€â”€ requirements.gpu.txt         # GPU dependencies
â””â”€â”€ models/                      # Cache modelos Whisper
```

## ğŸ¯ Benchmarks Performance

| Modo | Ãudio 5min | Qualidade | Custo | Privacidade |
|------|------------|-----------|-------|-------------|
| **RTX 3060** | ~30s | 95%+ PT-BR | GrÃ¡tis | 100% Local |
| Groq API | ~45s | 90% PT-BR | Free tier | Cloud |
| Google Cloud | ~60s | 92% PT-BR | Free tier | Cloud |

## ğŸ”§ ConfiguraÃ§Ã£o AvanÃ§ada

### **OtimizaÃ§Ã£o GPU**
```env
# .env para RTX 3060
TORCH_CUDA_ARCH_LIST="8.6"
CUDA_VISIBLE_DEVICES=0
WHISPER_MODEL=large-v3
WHISPER_DEVICE=cuda

# Para GPUs com pouca VRAM
WHISPER_MODEL=medium
TORCH_PRECISION=fp16
```

### **Fallback AutomÃ¡tico**
O sistema tenta na ordem:
1. **Local GPU** (se disponÃ­vel)
2. **Groq API** (se configurado)  
3. **Google Cloud** (se configurado)
4. **HuggingFace** (backup)

## ğŸ“¤ Deploy ProduÃ§Ã£o

### **Google Cloud Run**
```bash
# Build para produÃ§Ã£o (sem GPU)
docker build -f Dockerfile -t cloud-transcript .
gcloud run deploy --image cloud-transcript
```

### **VPS Ubuntu 22.04**
```bash
# Deploy em vps.frontzin.com.br
./deploy/vps-deploy.sh
```

## ğŸ¤ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie branch: `git checkout -b feature/nova-feature`
3. Commit: `git commit -m 'Add nova feature'`
4. Push: `git push origin feature/nova-feature`
5. Abra Pull Request

## ğŸ“„ LicenÃ§a

MIT License - veja [LICENSE](LICENSE)

## ğŸ†˜ Suporte

- ğŸŒ **Website**: [IA Forte](https://iaforte.com.br)
- ğŸ“§ **Email**: ecodelearn@outlook.com
- ğŸ› **Issues**: [GitHub Issues](https://github.com/ecodelearn/cloud_transcript/issues)

---

**âš¡ Powered by RTX 3060 + desenvolvido com â¤ï¸ pela [IA Forte](https://iaforte.com.br)**