services:
  app:
    build:
      context: .
      dockerfile: Dockerfile.gpu
    container_name: cloud_transcript_gpu
    ports:
      - "8501:8501"
    volumes:
      # Hot-reload: monta código fonte
      - ./src:/app/src:ro
      # Volumes para persistência de dados
      - ./uploads:/app/uploads
      - ./exports:/app/exports  
      - ./cache:/app/cache
      - ./logs:/app/logs
      # Cache de modelos Whisper (importante para GPU)
      - ./models:/app/models
    environment:
      - PYTHONPATH=/app
      - STREAMLIT_SERVER_PORT=8501
      - STREAMLIT_SERVER_ADDRESS=0.0.0.0
      - STREAMLIT_SERVER_WATCH_FOR_CHANGES=true
      - STREAMLIT_SERVER_RUN_ON_SAVE=true
      # GPU Configuration
      - CUDA_VISIBLE_DEVICES=0
      - USE_LOCAL_GPU=true
      - WHISPER_MODEL=large-v3
      - WHISPER_DEVICE=cuda
      - TORCH_DEVICE=cuda
    env_file:
      - .env
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - cloud_transcript_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8501/_stcore/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s  # Maior tempo para download de modelos

networks:
  cloud_transcript_network:
    driver: bridge

volumes:
  uploads:
    driver: local
  exports:
    driver: local
  cache:
    driver: local
  logs:
    driver: local
  models:
    driver: local